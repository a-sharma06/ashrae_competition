{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, LeakyReLU\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Concatenate, Flatten, Reshape, Lambda\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, LSTM\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"gs://123test_bucket/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading = train['meter_reading']\n",
    "train['meter_reading'] = np.log1p(train['meter_reading'])\n",
    "\n",
    "scaler1 = MinMaxScaler()\n",
    "train['meter_reading'] = scaler1.fit_transform(X=np.reshape(train['meter_reading'].values, (-1, 1))).reshape(len(train),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train = pd.read_csv(\"gs://123test_bucket/weather_train.csv\")\n",
    "meta = pd.read_csv(\"gs://123test_bucket/building_metadata.csv\")\n",
    "\n",
    "train.timestamp = pd.to_datetime(train.timestamp)\n",
    "weather_train.timestamp = pd.to_datetime(weather_train.timestamp)\n",
    "\n",
    "weather_train['month'] = weather_train.timestamp.dt.month\n",
    "weather_train['dayofweek'] = weather_train.timestamp.dt.dayofweek\n",
    "weather_train['hour'] = weather_train.timestamp.dt.hour\n",
    "\n",
    "train_meta_w = pd.merge(weather_train, meta, on='site_id')\n",
    "\n",
    "train_meta_w = train_meta_w.fillna(0)\n",
    "\n",
    "train = pd.merge(train, train_meta_w, on=['building_id', 'timestamp'], how='inner') # we have enough training data\n",
    "train_meta_w = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None, drop='first',\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "              n_values=None, sparse=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(drop='first', sparse=False)\n",
    "cat_cols = ['site_id', 'hour', 'dayofweek', 'month', 'primary_use', 'year_built']\n",
    "other_cols = ['building_id', 'meter']\n",
    "enc.fit(train[cat_cols + other_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23]),\n",
       " array([0, 1, 2, 3, 4, 5, 6]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]),\n",
       " array(['Education', 'Entertainment/public assembly',\n",
       "        'Food sales and service', 'Healthcare', 'Lodging/residential',\n",
       "        'Manufacturing/industrial', 'Office', 'Other', 'Parking',\n",
       "        'Public services', 'Religious worship', 'Retail', 'Services',\n",
       "        'Technology/science', 'Utility', 'Warehouse/storage'], dtype=object),\n",
       " array([   0., 1900., 1902., 1903., 1904., 1905., 1906., 1907., 1908.,\n",
       "        1909., 1910., 1911., 1912., 1913., 1914., 1915., 1916., 1917.,\n",
       "        1918., 1919., 1920., 1921., 1922., 1923., 1924., 1925., 1926.,\n",
       "        1927., 1928., 1929., 1930., 1931., 1932., 1933., 1934., 1935.,\n",
       "        1936., 1937., 1938., 1939., 1940., 1941., 1942., 1944., 1945.,\n",
       "        1946., 1947., 1948., 1949., 1950., 1951., 1952., 1953., 1954.,\n",
       "        1955., 1956., 1957., 1958., 1959., 1960., 1961., 1962., 1963.,\n",
       "        1964., 1965., 1966., 1967., 1968., 1969., 1970., 1971., 1972.,\n",
       "        1973., 1974., 1975., 1976., 1977., 1978., 1979., 1980., 1981.,\n",
       "        1982., 1983., 1984., 1985., 1986., 1987., 1988., 1989., 1990.,\n",
       "        1991., 1992., 1993., 1994., 1995., 1996., 1997., 1998., 1999.,\n",
       "        2000., 2001., 2002., 2003., 2004., 2005., 2006., 2007., 2008.,\n",
       "        2009., 2010., 2011., 2012., 2013., 2014., 2015., 2016., 2017.]),\n",
       " array([   0,    1,    2, ..., 1446, 1447, 1448]),\n",
       " array([0, 1, 2, 3])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, dt, num_cols, cat_names, new_cat_names, batch_size = 10):\n",
    "        self.batch_size = batch_size\n",
    "        self.len = len(dt)//batch_size\n",
    "        self.dt = dt[num_cols + cat_names + ['meter_reading']]\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_names = cat_names\n",
    "        self.new_cat_names = new_cat_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_start = index*self.batch_size\n",
    "        index_end = index_start+self.batch_size\n",
    "        adf = self.dt[index_start:index_end]\n",
    "        cat_vals = enc.transform(adf[self.cat_names])\n",
    "        return np.concatenate([adf[self.num_cols].values, cat_vals], axis =1), adf['meter_reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['air_temperature', 'cloud_coverage', 'dew_temperature',\n",
    "       'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
    "       'wind_speed', 'square_feet', 'floor_count']\n",
    "\n",
    "cat_names = cat_cols + other_cols\n",
    "new_cat_names = list(enc.get_feature_names(cat_names))\n",
    "col_names = num_cols + new_cat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.0, 6.0, 20.0, 0.0, 1019.7, 0.0, 0.0, 7432.0, 0.0, 0, 0, 4, 1,\n",
       "        'Education', 2008.0, 0, 0],\n",
       "       [25.0, 6.0, 20.0, 0.0, 1019.7, 0.0, 0.0, 2720.0, 0.0, 0, 0, 4, 1,\n",
       "        'Education', 2004.0, 1, 0],\n",
       "       [25.0, 6.0, 20.0, 0.0, 1019.7, 0.0, 0.0, 5376.0, 0.0, 0, 0, 4, 1,\n",
       "        'Education', 1991.0, 2, 0],\n",
       "       [25.0, 6.0, 20.0, 0.0, 1019.7, 0.0, 0.0, 23685.0, 0.0, 0, 0, 4, 1,\n",
       "        'Education', 2002.0, 3, 0],\n",
       "       [25.0, 6.0, 20.0, 0.0, 1019.7, 0.0, 0.0, 116607.0, 0.0, 0, 0, 4,\n",
       "        1, 'Education', 1975.0, 4, 0]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([train.head()[num_cols].values, train.head()[cat_names].values], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = train['building_id'].unique()\n",
    "len_val = round(len(ids)*0.3)\n",
    "ids_val = np.random.choice(ids, len_val, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024#1024+512\n",
    "train_gen = DataGenerator(train, \n",
    "                          num_cols=num_cols, \n",
    "                          cat_names=cat_names,\n",
    "                          new_cat_names = new_cat_names,\n",
    "                          batch_size=batch_size)\n",
    "val_gen = DataGenerator(train[train['building_id'].isin(ids_val)], \n",
    "                          num_cols=num_cols, \n",
    "                          cat_names=cat_names,\n",
    "                          new_cat_names = new_cat_names,\n",
    "                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1646)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum = train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform(dum[cat_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['site_id_1', 'site_id_2', 'site_id_3', ..., 'meter_1', 'meter_2',\n",
       "       'meter_3'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.get_feature_names(cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "filters = 10\n",
    "ncols = train_gen[0][0].shape[1] # categorical columns\n",
    "states = 20\n",
    "act = 'linear'\n",
    "\n",
    "input_shape1 = Input(shape=(ncols,))\n",
    "\n",
    "m1 = Dense(200, activation=act)(input_shape1)\n",
    "m1 = Dropout(0.4)(m1)\n",
    "m1 = Dense(10, activation=act)(m1)\n",
    "m1 = LeakyReLU(alpha=0.1)(m1)\n",
    "m1 = Dropout(0.2)(m1)\n",
    "m1 = Dense(1, activation='sigmoid')(m1)\n",
    "\n",
    "model2 = Model(inputs = input_shape1, outputs = m1)\n",
    "model2._make_predict_function()\n",
    "\n",
    "opt = keras.optimizers.RMSprop(clipnorm=1.)\n",
    "\n",
    "model2.compile(loss=root_mean_squared_error, optimizer=opt, metrics=['mse', 'mae', 'mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19653/19653 [==============================] - 758s 39ms/step - loss: 0.2756 - mse: 0.0765 - mae: 0.2455 - mape: 34944.3633 - val_loss: 0.2549 - val_mse: 0.0732 - val_mae: 0.2386 - val_mape: 90.5200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f56042feb38>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "epochs = 1#TODO: change it back to 5\n",
    "workers = 10\n",
    "model2.fit_generator(generator=train_gen,\n",
    "                    validation_data=val_gen, epochs=epochs, \n",
    "                    use_multiprocessing=True, workers = workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding: No improvment\n",
    "# First layer with 200 nodes, epoch1=0.2754\n",
    "# First layer with 400 nodes, epoch1=0.2753\n",
    "# First layer with 200 nodes followed by Dropout(0.4), epoch1=0.2756\n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
