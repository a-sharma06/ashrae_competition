{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow\n",
    "import crcmod\n",
    "\n",
    "#try pip install --user --upgrade scikit-learn==0.20\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, LeakyReLU\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Concatenate, Flatten, Reshape, Lambda\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, LSTM\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  meter  meter_reading            timestamp\n",
       "0            0    0.0            0.0  2016-01-01 00:00:00\n",
       "1            1    0.0            0.0  2016-01-01 00:00:00\n",
       "2            2    0.0            0.0  2016-01-01 00:00:00\n",
       "3            3    0.0            0.0  2016-01-01 00:00:00\n",
       "4            4    0.0            0.0  2016-01-01 00:00:00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Todo: \n",
    "# Correct site 0 values: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/119261\n",
    "# Leak: https://www.kaggle.com/yamsam/ashrae-leak-data-station\n",
    "# https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature\n",
    "# https://www.kaggle.com/datadugong/locate-better-cities-by-weather-temp-fill-nans\n",
    "# https://www.kaggle.com/jimeno/ashrae-weather-features-are-not-in-local-time\n",
    "site_dict ={0:'UCF-Orlando',\n",
    "           1:'UCL-London',\n",
    "           2:'ASU-Tempe',\n",
    "            3:'DC',\n",
    "           4:'UCB-Berkeley',\n",
    "            5:'London/Birmingham/Philly',\n",
    "            6:'DC',\n",
    "            7:'Ottawa',\n",
    "            8:'UCF-Orlando',\n",
    "            9:'San Antonio',\n",
    "            10:'Salt Lake City',\n",
    "            11:'Ottawa',\n",
    "            12:'Dublin',\n",
    "            13:'Minneapolis',\n",
    "            14:'Philadelphia',\n",
    "           15:'Cornell',\n",
    "           }\n",
    "\n",
    "#pip install --user feather-format\n",
    "leak_df = pd.read_csv(\"gs://inputdata23/leak.csv\", index_col=0)\n",
    "leak_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "leak_df.fillna(0, inplace=True)\n",
    "leak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0\n",
    "leak_df.timestamp = pd.to_datetime(leak_df.timestamp)\n",
    "leak_df = leak_df[(leak_df.timestamp.dt.year > 2016) & (leak_df.timestamp.dt.year < 2019)]\n",
    "print (leak_df.duplicated().sum())\n",
    "#hi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful links:\n",
    "1. Inverse of np.log1p: https://stackoverflow.com/questions/50049891/what-is-the-inverse-of-numpys-log1p\n",
    "2. Building parallel keras model: https://stackoverflow.com/questions/43151775/how-to-have-parallel-convolutional-layers-in-keras\n",
    "3. How to make parallel keras models: https://datascience.stackexchange.com/questions/39407/how-to-make-two-parallel-convolutional-neural-networks-in-keras\n",
    "4. 2 parallel keras layers: https://stackoverflow.com/questions/51546075/two-parallel-conv2d-layers-keras\n",
    "5. Setting up Conv1D and LSTM: https://stackoverflow.com/questions/51344610/how-to-setup-1d-convolution-and-lstm-in-keras\n",
    "6. Take average of LSTM hidden states using Lambda: https://stackoverflow.com/questions/51479940/average-channels-of-convolutional-layer-keras\n",
    "7. Preparing categorical variable for neural networks: https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/\n",
    "8. Tutorial for using keras for time series (no generators used): https://medium.com/@jdwittenauer/deep-learning-with-keras-structured-time-series-37a66c6aeb28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  meter  meter_reading            timestamp\n",
       "0            0    0.0            0.0  2016-01-01 00:00:00\n",
       "1            1    0.0            0.0  2016-01-01 00:00:00\n",
       "2            2    0.0            0.0  2016-01-01 00:00:00\n",
       "3            3    0.0            0.0  2016-01-01 00:00:00\n",
       "4            4    0.0            0.0  2016-01-01 00:00:00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"gs://inputdata23/train.csv\")\n",
    "\n",
    "def _drop_electrical_zeros(X, y):\n",
    "    X = X[(y > 0) | (X.meter != 0)]\n",
    "    y = y.reindex(X.index)\n",
    "    return X, y\n",
    "\n",
    "def _drop_missing_site_0(X, y):\n",
    "    X = X[(X.timestamp >= 3378) | (X.site_id != 0) | (X.meter != 0)]\n",
    "    y = y.reindex(X.index)\n",
    "    return X, y\n",
    "\n",
    "X, y = [(X, y) for (X, y) in train.iterrows()]\n",
    "\n",
    "train = _drop_electrical_zeros(X, y)\n",
    "\n",
    "frames = [train, leak_df]\n",
    "train = pd.concat(frames, sort=True)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/purist1024/strategy-evaluation-what-helps-and-by-how-much/comments\n",
    "Drop potentially bad data\n",
    "There are two different optional strategies here that can be used together. _drop_electrical_zeros removes every electrical meter reading with a value of 0; while _drop_missing_site_0 gets rid of all electrical meter readings for site zero during the 141 days that its readings were mostly pegged to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3326\u001b[0;31m                     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3327\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8306d851b925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8306d851b925>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    748\u001b[0m     ):\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0minferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minferred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"period\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.infer_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib._try_infer_map\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_name_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;31m# provides dtype.name.__get__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Meter Reading:\n",
    "1. nplog1p\n",
    "2. Group min-max scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading = train['meter_reading']\n",
    "train['meter_reading'] = np.log1p(train['meter_reading'])\n",
    "\n",
    "#scaler1 = MinMaxScaler()\n",
    "#train['meter_reading'] = scaler1.fit_transform(X=np.reshape(train['meter_reading'].values, (-1, 1))).reshape(len(train),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['key'] = train['building_id'].astype(str) + train['meter'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_dict = dict()\n",
    "\n",
    "def minmaxscaler(x, name, l):\n",
    "    #print(name)\n",
    "    key = name\n",
    "    sc_dict[key]=MinMaxScaler()\n",
    "    return sc_dict[key].fit_transform(X=np.reshape(x.values, (-1, 1))).reshape(l,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['meter_reading'] = train.groupby('key')['meter_reading'].transform(lambda x: minmaxscaler(x, x.name, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sc_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in train['key'].unique():\n",
    "#     print(key)\n",
    "#     l = len(train.loc[train.key == key])\n",
    "#     scaler1 = MinMaxScaler()\n",
    "#     train.loc[train.key == key, 'meter_reading'] = scaler1.fit_transform(X=np.reshape(train.loc[train.key == key, 'meter_reading'].values, (-1, 1))).reshape(l,)\n",
    "#     sc_dict[key] = scaler1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['meter_reading'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"gs://inputdata23/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.timestamp = pd.to_datetime(train.timestamp)\n",
    "test.timestamp = pd.to_datetime(test.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Weather and Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train = pd.read_csv(\"gs://inputdata23/weather_train.csv\")\n",
    "weather_test = pd.read_csv(\"gs://inputdata23/weather_test.csv\")\n",
    "meta = pd.read_csv(\"gs://inputdata23/building_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train.timestamp = pd.to_datetime(weather_train.timestamp)\n",
    "weather_test.timestamp = pd.to_datetime(weather_test.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep = ['site_id', 'timestamp','air_temperature', 'dew_temperature']#, 'wind_speed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching last hour of weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train = weather_train.sort_values(by = 'timestamp')\n",
    "weather_test = weather_test.sort_values(by = 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather_train.append(weather_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather['air_temperature_1'] = weather.groupby(['site_id'])['air_temperature'].shift(1)\n",
    "#weather['dew_temperature_1'] = weather.groupby(['site_id'])['dew_temperature'].shift(1)\n",
    "#weather['wind_speed_1'] = weather.groupby(['site_id'])['wind_speed'].shift(1)\n",
    "\n",
    "#weather['air_temperature_2'] = weather.groupby(['site_id'])['air_temperature'].shift(2)\n",
    "#weather['dew_temperature_2'] = weather.groupby(['site_id'])['dew_temperature'].shift(2)\n",
    "#weather['wind_speed_1'] = weather.groupby(['site_id'])['wind_speed'].shift(1)\n",
    "\n",
    "#weather['air_temperature_3'] = weather.groupby(['site_id'])['air_temperature'].shift(3)\n",
    "#weather['dew_temperature_3'] = weather.groupby(['site_id'])['dew_temperature'].shift(3)\n",
    "#weather['wind_speed_1'] = weather.groupby(['site_id'])['wind_speed'].shift(1)\n",
    "\n",
    "#weather['air_temperature_4'] = weather.groupby(['site_id'])['air_temperature'].shift(4)\n",
    "#weather['dew_temperature_4'] = weather.groupby(['site_id'])['dew_temperature'].shift(4)\n",
    "#weather['wind_speed_1'] = weather.groupby(['site_id'])['wind_speed'].shift(1)\n",
    "\n",
    "#weather['air_temperature_5'] = weather.groupby(['site_id'])['air_temperature'].shift(5)\n",
    "#weather['dew_temperature_5'] = weather.groupby(['site_id'])['dew_temperature'].shift(5)\n",
    "#weather['wind_speed_1'] = weather.groupby(['site_id'])['wind_speed'].shift(1)\n",
    "\n",
    "#weather['air_temperature_6'] = weather.groupby(['site_id'])['air_temperature'].shift(6)\n",
    "#weather['dew_temperature_6'] = weather.groupby(['site_id'])['dew_temperature'].shift(6)\n",
    "#weather['wind_speed_1'] = weather.groupby(['site_id'])['wind_speed'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep = cols_keep #+ ['air_temperature_1',# 'dew_temperature_1',\n",
    "#                         'air_temperature_2',# 'dew_temperature_2',\n",
    "#                         'air_temperature_3',# 'dew_temperature_3',\n",
    "#                         'air_temperature_4', #'dew_temperature_4',\n",
    "#                         'air_temperature_5', #'dew_temperature_5',\n",
    "#                         'air_temperature_6']#, #'dew_temperature_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(pd.merge(train, meta, on='building_id', how='left'),\n",
    "                 weather[cols_keep],\n",
    "                 on=['site_id', 'timestamp'],\n",
    "                 how='inner')\n",
    "test = pd.merge(pd.merge(test, meta, on='building_id', how='left'),\n",
    "                 weather[cols_keep],\n",
    "                 on=['site_id', 'timestamp'],\n",
    "                 how='left')\n",
    "\n",
    "### site 0 from kBtu to kWh\n",
    "train.meter_reading[(train.site_id == 0)] = train.meter_reading*0.2931\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['month'] = train.timestamp.dt.month\n",
    "train['dayofweek'] = train.timestamp.dt.dayofweek\n",
    "train['hour'] = train.timestamp.dt.hour\n",
    "\n",
    "test['month'] = test.timestamp.dt.month\n",
    "test['dayofweek'] = test.timestamp.dt.dayofweek\n",
    "test['hour'] = test.timestamp.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Weekday/Weekend Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['weekday'] = 0\n",
    "test['weekday'] = 0\n",
    "weekday_map = {0:0,\n",
    "               1:0,\n",
    "               2:0,\n",
    "               3:0,\n",
    "               4:0,\n",
    "               5:1,\n",
    "               6:1}\n",
    "train['weekday'] = train['dayofweek'].map(weekday_map)\n",
    "test['weekday'] = test['dayofweek'].map(weekday_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[40525*1024: (40525*1024)+1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = MinMaxScaler()\n",
    "num_cols = ['air_temperature', 'dew_temperature'] + ['square_feet', 'floor_count']#+['wind_speed']# + ['air_temperature_1', \n",
    "                                                                                                    #'dew_temperature_1',\n",
    "                                                                                                    #'air_temperature_2', \n",
    "                                                                                                    #'dew_temperature_2',\n",
    "                                                                                                    #'air_temperature_3',\n",
    "                                                                                                    #'dew_temperature_3',\n",
    "                                                                                                    #'air_temperature_4', #'dew_temperature_4',\n",
    "                                                                                                    #'air_temperature_5', #'dew_temperature_5',\n",
    "                                                                                                    #'air_temperature_6']#, #'dew_temperature_6']\n",
    "scaler2.fit(train[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[num_cols] = scaler2.transform(train[num_cols])\n",
    "test[num_cols] = scaler2.transform(test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Weather, Metadata with load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "cat_cols = ['site_id', 'hour', 'dayofweek', 'month', 'primary_use']#, 'year_built']\n",
    "other_cols = ['building_id', 'meter']\n",
    "enc.fit(train[cat_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[cat_cols] = enc.transform(train[cat_cols])\n",
    "test[cat_cols] = enc.transform(test[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random ID selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 30% of the data as validation data. If more data is needed, we should consider adding data by randomly selecting buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = train['building_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping 80% of the ids\n",
    "len_sub = round(len(ids)*0.80)\n",
    "ids_sub = np.random.choice(ids, len_sub, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.3 represents the percentage of data that is kept for validation\n",
    "len_val = round(len(ids_sub)*0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_val = np.random.choice(ids_sub, len_val, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = np.setdiff1d(ids_sub,ids_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ids_val)+len(ids_train)==len(ids_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying TimeseriesGenerator to the ASHRAE training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once difference between the example above, and our situation is that we have multiple timeseries, for each building and each meter in the building. So, we will be required to modify the code a little bit.\n",
    "\n",
    "Below, we check how many meters exist in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train[['building_id', 'meter']].drop_duplicates()), len(test[['building_id', 'meter']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below has been taken from this stackoverflow answer with some modifications:\n",
    "https://stackoverflow.com/questions/55116638/use-keras-timeseriesgenerator-function-to-generate-squence-group-by-some-id/55118459#55118459\n",
    "\n",
    "The modification is basically that once we subset the data for building ID, it is then subset for meter type also.\n",
    "\n",
    "Further reading about modifying keras generator classes can be found below:\n",
    "https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = num_cols\n",
    "\n",
    "cat_names = cat_cols + other_cols + ['weekday']\n",
    "\n",
    "#new_cat_names = list(enc.get_feature_names(cat_names))\n",
    "\n",
    "#col_names = num_cols + new_cat_names\n",
    "cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, dt, num_cols, cat_names, batch_size = 10):\n",
    "        self.batch_size = batch_size\n",
    "        self.len = len(dt)//batch_size\n",
    "        self.dt = dt[num_cols + cat_names + ['meter_reading']]\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_names = cat_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_start = index*self.batch_size\n",
    "        index_end = index_start+self.batch_size\n",
    "        adf = self.dt[index_start:index_end]\n",
    "        return [adf[x] for x in cat_names]+[adf[num_cols]], adf['meter_reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, dt, num_cols, cat_names, batch_size = 10):\n",
    "        self.batch_size = batch_size\n",
    "        self.len = round(len(dt)/batch_size)\n",
    "        self.dt = dt[num_cols + cat_names]\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_names = cat_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_start = index*self.batch_size\n",
    "        index_end = index_start+self.batch_size\n",
    "        adf = self.dt[index_start:index_end]\n",
    "        return [adf[x] for x in cat_names]+[adf[num_cols]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "batch_size = 60*1024#1024+512\n",
    "train_gen = DataGenerator(train[train['building_id'].isin(ids_train)], \n",
    "                          num_cols=num_cols, \n",
    "                          cat_names=cat_names,\n",
    "                          batch_size=batch_size)\n",
    "val_gen = DataGenerator(train[train['building_id'].isin(ids_val)],\n",
    "                        num_cols=num_cols, \n",
    "                        cat_names=cat_names,\n",
    "                        batch_size=batch_size)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check1 = train_gen[0]\n",
    "check2 = val_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(check1[0]), len(check2[0]), len(cat_names)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function - Root Mean Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings NN\n",
    "\n",
    "1. https://towardsdatascience.com/decoded-entity-embeddings-of-categorical-variables-in-neural-networks-1d2468311635\n",
    "2. https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9\n",
    "3. https://github.com/mayanksatnalika/ipython/blob/master/embeddings%20project/cycle_sharing/entity_embeddings_regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_names), len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "inputs = []\n",
    "act = 'relu'\n",
    "n_num_cols = len(num_cols)\n",
    "\n",
    "\n",
    "for categorical_var in cat_names:\n",
    "    model = Sequential()\n",
    "    model.reset_states()\n",
    "    no_of_unique_cat  = train[categorical_var].nunique()\n",
    "    embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "    embedding_size = int(embedding_size)\n",
    "    input1 = Input(shape=(1,))\n",
    "    model = Embedding(no_of_unique_cat+1,embedding_size)(input1)\n",
    "    model = Reshape(target_shape=(embedding_size,))(model)\n",
    "    models.append(model)\n",
    "    inputs.append(input1)\n",
    "    \n",
    "input1 = Input(shape=(n_num_cols,))\n",
    "model_rest = Dense(100, activation=act)(input1)\n",
    "models.append(model_rest)\n",
    "inputs.append(input1)\n",
    "\n",
    "m1 = keras.layers.concatenate(models, axis = 1)\n",
    "m1 = Dropout(0.5)(m1)\n",
    "m1 = Dense(200, activation=act)(m1)\n",
    "m1 = Dropout(0.5)(m1)\n",
    "m1 = Dense(5, activation=act)(m1)\n",
    "m1 = LeakyReLU(alpha=0.1)(m1)\n",
    "m1 = Dropout(0.2)(m1)\n",
    "m1 = Dense(1, activation='sigmoid')(m1)\n",
    "\n",
    "model3 = Model(inputs = inputs, outputs = m1)\n",
    "\n",
    "opt = keras.optimizers.RMSprop(clipnorm=1.)\n",
    "\n",
    "model3.compile(loss=root_mean_squared_error, optimizer=opt, metrics=['mse', 'mae', 'mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10#TODO: change it back to 5\n",
    "workers = 10\n",
    "model3.fit_generator(generator=train_gen,\n",
    "                    validation_data=val_gen, epochs=epochs, \n",
    "                    use_multiprocessing=True, workers = workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_rest with Dense(65), epoch 1: loss=0.2594, val_loss=0.2829, val_loss somewhat constant\n",
    "# model_rest with Dense(120), epoch 1: loss=0.2589, val_loss=0.2835, val_loss somewhat constant\n",
    "# model_rest with Dense(300), epoch 1: loss=0.2594, val_loss=0.2830, val_loss somewhat constant\n",
    "# model_rest with Dense(65), combined model with Dense(200), epoch 1: loss=0.2554, val_loss=0.2858, val_loss reduces to 0.2550 by epoch 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dense(64) for model_rest, epoch 1: loss=0.0656, val_loss=0.1382, val_loss increased\n",
    "2. Replacing the Dense(64) for model_rest with Dense(24), epoch 1: loss: 0.0648, val_loss: 0.1481, val_loss increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_gen = TestDataGenerator(train[train['building_id'].isin(ids_val[0:20])], \n",
    "                                 num_cols=num_cols, \n",
    "                                cat_names=cat_names,\n",
    "                                batch_size=batch_size)\n",
    "print(len(val_test_gen))\n",
    "\n",
    "val_test_res = model3.predict_generator(generator=val_test_gen, workers=12, use_multiprocessing=True)\n",
    "\n",
    "x = []\n",
    "for i in range(len(val_test_gen)):\n",
    "    x.append(val_gen[i][1].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = train[train['building_id'].isin(ids_val[0:20])].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy['y'] = val_test_res\n",
    "dummy['key'] = dummy['building_id'].astype(str) + dummy['meter'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseminmax(x, name, l):\n",
    "    key = name\n",
    "    return sc_dict[key].inverse_transform(np.reshape(x.values, (-1, 1))).reshape(l,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_y = np.expm1(dummy.groupby('key')['y'].transform(lambda x: inverseminmax(x, x.name, len(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(val_test_y) == len(val_test_res_og))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error: \" +str({np.sqrt(np.mean(np.square(val_test_res_og - val_test_y)))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Function for Prediction - Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = TestDataGenerator(test, num_cols=num_cols, \n",
    "                        cat_names=cat_names,\n",
    "                        batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.loc[40525*1024: (40525*1024)+1024]#.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model3.predict_generator(generator=test_gen, workers=12, use_multiprocessing=True, verbose=1)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result# = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(np.expm1(scaler1.inverse_transform(result)).reshape(result.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pd.read_csv(\"gs://123test_bucket/test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "41697600/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test.loc[0:10,'building_id'].astype(str) + test.loc[0:10,'meter'].astype(str)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df['key'] = (test['building_id'].astype(str) + test['meter'].astype(str)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['meter_reading']=result\n",
    "result_df['row_id']=test['row_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['meter_reading'] = np.expm1(result_df.groupby('key')['meter_reading'].transform(lambda x: inverseminmax(x, x.name, len(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df[['row_id', 'meter_reading']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"gs://inputdata23/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 6)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = pd.read_csv(\"result.csv\")\n",
    "check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, meter_reading]\n",
       "Index: []"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[result_df.meter_reading.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"gs://123test_bucket/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['meter_reading'] = result_df['meter_reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(\"result_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model2.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model2.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
