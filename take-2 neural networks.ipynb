{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.close()\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask.distributed import Client, LocalCluster\n",
    "#cluster = LocalCluster()\n",
    "#client = Client(cluster)\n",
    "# client = Client(n_workers=4, threads_per_worker=4, memory_limit='12GB')\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, LeakyReLU\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Concatenate, Flatten, Reshape, Lambda\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, LSTM\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful links:\n",
    "1. Inverse of np.log1p: https://stackoverflow.com/questions/50049891/what-is-the-inverse-of-numpys-log1p\n",
    "2. Building parallel keras model: https://stackoverflow.com/questions/43151775/how-to-have-parallel-convolutional-layers-in-keras\n",
    "3. How to make parallel keras models: https://datascience.stackexchange.com/questions/39407/how-to-make-two-parallel-convolutional-neural-networks-in-keras\n",
    "4. 2 parallel keras layers: https://stackoverflow.com/questions/51546075/two-parallel-conv2d-layers-keras\n",
    "5. Setting up Conv1D and LSTM: https://stackoverflow.com/questions/51344610/how-to-setup-1d-convolution-and-lstm-in-keras\n",
    "6. Take average of LSTM hidden states using Lambda: https://stackoverflow.com/questions/51479940/average-channels-of-convolutional-layer-keras\n",
    "7. Preparing categorical variable for neural networks: https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/\n",
    "8. Tutorial for using keras for time series (no generators used): https://medium.com/@jdwittenauer/deep-learning-with-keras-structured-time-series-37a66c6aeb28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"gs://123test_bucket/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading = train['meter_reading']\n",
    "train['meter_reading'] = np.log1p(train['meter_reading'])\n",
    "\n",
    "scaler1 = MinMaxScaler()\n",
    "train['meter_reading'] = scaler1.fit_transform(X=np.reshape(train['meter_reading'].values, (-1, 1))).reshape(len(train),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2452794762030242"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['meter_reading'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"gs://123test_bucket/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining Month, Day of the Week, and Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.timestamp = pd.to_datetime(train.timestamp)\n",
    "\n",
    "test.timestamp = pd.to_datetime(test.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20216100, 41697600)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train = pd.read_csv(\"gs://123test_bucket/weather_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test = pd.read_csv(\"gs://123test_bucket/weather_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"gs://123test_bucket/building_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train.timestamp = pd.to_datetime(weather_train.timestamp)\n",
    "\n",
    "weather_test.timestamp = pd.to_datetime(weather_test.timestamp)\n",
    "\n",
    "weather_train['month'] = weather_train.timestamp.dt.month\n",
    "weather_train['dayofweek'] = weather_train.timestamp.dt.dayofweek\n",
    "weather_train['hour'] = weather_train.timestamp.dt.hour\n",
    "\n",
    "weather_test['month'] = weather_test.timestamp.dt.month\n",
    "weather_test['dayofweek'] = weather_test.timestamp.dt.dayofweek\n",
    "weather_test['hour'] = weather_test.timestamp.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_w = pd.merge(weather_train, meta, on='site_id')\n",
    "test_meta_w = pd.merge(weather_test, meta, on='site_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_w = train_meta_w.fillna(0)\n",
    "test_meta_w = test_meta_w.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler2 = MinMaxScaler()\n",
    "num_cols = ['air_temperature', 'cloud_coverage', 'dew_temperature',\n",
    "       'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
    "       'wind_speed', 'square_feet', 'floor_count']\n",
    "scaler2.fit(train_meta_w[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_w[num_cols] = scaler2.transform(train_meta_w[num_cols])\n",
    "test_meta_w[num_cols] = scaler2.transform(test_meta_w[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "cat_cols = ['site_id', 'hour', 'dayofweek', 'month', 'primary_use', 'year_built']\n",
    "other_cols = ['building_id', 'meter']\n",
    "enc.fit(train_meta_w[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_w[cat_cols] = enc.transform(train_meta_w[cat_cols])\n",
    "test_meta_w[cat_cols] = enc.transform(test_meta_w[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Weather, Metadata with load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(train_meta_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, train_meta_w, on=['building_id', 'timestamp'], how='inner') # we have enough training data\n",
    "train_meta_w = None\n",
    "test = pd.merge(test, test_meta_w, on=['building_id', 'timestamp'], how='left') # we can't lose any rows\n",
    "test_meta_w = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random ID selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 30% of the data as validation data. If more data is needed, we should consider adding data by randomly selecting buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = train['building_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only 5% of the ids\n",
    "len_sub = round(len(ids)*0.05)\n",
    "ids_sub = np.random.choice(ids, len_sub, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.3 represents the percentage of data that is kept for validation\n",
    "len_val = round(len(ids_sub)*0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_val = np.random.choice(ids_sub, len_val, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = np.setdiff1d(ids_sub,ids_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ids_val)+len(ids_train)==len(ids_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying TimeseriesGenerator to the ASHRAE training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once difference between the example above, and our situation is that we have multiple timeseries, for each building and each meter in the building. So, we will be required to modify the code a little bit.\n",
    "\n",
    "Below, we check how many meters exist in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train[['building_id', 'meter']].drop_duplicates()), len(test[['building_id', 'meter']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below has been taken from this stackoverflow answer with some modifications:\n",
    "https://stackoverflow.com/questions/55116638/use-keras-timeseriesgenerator-function-to-generate-squence-group-by-some-id/55118459#55118459\n",
    "\n",
    "The modification is basically that once we subset the data for building ID, it is then subset for meter type also.\n",
    "\n",
    "Further reading about modifying keras generator classes can be found below:\n",
    "https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = num_cols + cat_cols + other_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, dt, col_names, batch_size = 10):\n",
    "        self.batch_size = batch_size\n",
    "        self.len = len(dt)//batch_size\n",
    "        self.dt = dt[col_names + ['meter_reading']]\n",
    "        self.col_names = col_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_start = index*self.batch_size\n",
    "        index_end = index_start+self.batch_size\n",
    "        adf = self.dt[index_start:index_end]\n",
    "        return adf[self.col_names], adf['meter_reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, dt, col_names, batch_size = 10):\n",
    "        self.batch_size = batch_size\n",
    "        self.len = len(dt)//batch_size\n",
    "        self.dt = dt[col_names]\n",
    "        self.col_names = col_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len+1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_start = index*self.batch_size\n",
    "        index_end = index_start+self.batch_size\n",
    "        adf = self.dt[index_start:index_end]\n",
    "        return adf[self.col_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "batch_size = 1024#1024+512\n",
    "train_gen = DataGenerator(train[train['building_id'].isin(ids_train)], col_names=col_names, batch_size=batch_size)\n",
    "val_gen = DataGenerator(train[train['building_id'].isin(ids_val)], col_names=col_names, batch_size=batch_size)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check1 = train_gen[0]\n",
    "check2 = val_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check1[0].shape, check1[1].shape, len(col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function - Root Mean Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "filters = 10\n",
    "ncols = len(col_names) # categorical columns\n",
    "states = 20\n",
    "act = 'linear'\n",
    "\n",
    "input_shape1 = Input(shape=(ncols,))\n",
    "\n",
    "m1 = Dense(10, activation=act)(input_shape1)\n",
    "m1 = Dense(5, activation=act)(m1)\n",
    "m1 = LeakyReLU(alpha=0.1)(m1)\n",
    "m1 = Dropout(0.2)(m1)\n",
    "m1 = Dense(1, activation='sigmoid')(m1)\n",
    "\n",
    "model2 = Model(inputs = input_shape1, outputs = m1)\n",
    "model2._make_predict_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(clipnorm=1.)\n",
    "\n",
    "model2.compile(loss=root_mean_squared_error, optimizer=opt, metrics=['mse', 'mae', 'mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(train['meter_reading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on dataset\n",
    "epochs = 1 #TODO: change it back to 5\n",
    "workers = 10\n",
    "model2.fit_generator(generator=train_gen,\n",
    "                    validation_data=val_gen, epochs=1, \n",
    "                    use_multiprocessing=True, workers = workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Function for Prediction - Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = TestDataGenerator(test, col_names=col_names, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model2.predict_generator(generator=test_gen, workers=workers, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00033766],\n",
       "       [0.00042102],\n",
       "       [0.00086147],\n",
       "       ...,\n",
       "       [0.0333986 ],\n",
       "       [0.02364207],\n",
       "       [0.02324256]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41498571, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             0.005724\n",
       "1             0.007142\n",
       "2             0.014667\n",
       "3             0.008076\n",
       "4             0.041769\n",
       "               ...    \n",
       "41498566     84.306885\n",
       "41498567    142.774689\n",
       "41498568      0.758586\n",
       "41498569      0.491233\n",
       "41498570      0.481197\n",
       "Length: 41498571, dtype: float32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.expm1(scaler1.inverse_transform(result)).reshape(result.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(pd.Series(np.expm1(scaler1.inverse_transform(result)).reshape(result.shape[0],)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41697600"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.read_csv(\"gs://123test_bucket/test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40720.3125"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "41697600/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
